{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy scipy\n",
    "#!pip install scikit-learn\n",
    "#!pip install pillow\n",
    "#!pip install h5py\n",
    "#!pip install tensorflow\n",
    "#!pip install tensorflow-gpu\n",
    "#!pip install keras\n",
    "#!pip install --upgrade --no-deps --force-reinstall tensorflow\n",
    "#!pip install ipywidgets\n",
    "#!pip install voila\n",
    "#!jupyter labextension install @jupyter-voila/jupyterlab-preview\n",
    "#!jupyter serverextension enable voila\n",
    "\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "import re\n",
    "import nltk # to process text data\n",
    "import numpy as np # to represent corpus as arrays\n",
    "import random \n",
    "import string # to process standard python strings\n",
    "from sklearn.metrics.pairwise import cosine_similarity # We will use this later to decide how similar two sentences are\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import HTML\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus=open('science NCERT CH2.txt','r+',errors = 'ignore')\n",
    "# raw_data=corpus.read()\n",
    "# raw_data = raw_data.lower()\n",
    "# sent_tokens = nltk.tokenize.sent_tokenize(raw_data)\n",
    "# corpus=open('science NCERT CH3.txt','r+',errors = 'ignore')\n",
    "# raw_data=corpus.read()\n",
    "# raw_data = raw_data.lower()\n",
    "# sent_tokens += nltk.tokenize.sent_tokenize(raw_data)\n",
    "# corpus=open('science NCERT CH4.txt','r+',errors = 'ignore')\n",
    "# raw_data=corpus.read()\n",
    "# raw_data = raw_data.lower()\n",
    "# sent_tokens += nltk.tokenize.sent_tokenize(raw_data)\n",
    "# corpus=open('science NCERT CH6.txt','r+',errors = 'ignore')\n",
    "# raw_data=corpus.read()\n",
    "# raw_data = raw_data.lower()\n",
    "# sent_tokens += nltk.tokenize.sent_tokenize(raw_data)\n",
    "# corpus=open('science NCERT CH7.txt','r+',errors = 'ignore')\n",
    "# raw_data=corpus.read()\n",
    "# raw_data = raw_data.lower()\n",
    "# sent_tokens += nltk.tokenize.sent_tokenize(raw_data)\n",
    "# corpus=open('science NCERT CH1.txt','r+',errors = 'ignore')\n",
    "# raw_data=corpus.read()\n",
    "# raw_data = raw_data.lower()\n",
    "# sent_tokens += nltk.tokenize.sent_tokenize(raw_data)\n",
    "# corpus=open('science NCERT CH8.txt','r+',errors = 'ignore')\n",
    "# raw_data=corpus.read()\n",
    "# raw_data = raw_data.lower()\n",
    "# sent_tokens += nltk.tokenize.sent_tokenize(raw_data)\n",
    "# corpus=open('science NCERT CH9.txt','r+',errors = 'ignore')\n",
    "# raw_data=corpus.read()\n",
    "# raw_data = raw_data.lower()\n",
    "# sent_tokens += nltk.tokenize.sent_tokenize(raw_data)\n",
    "# corpus=open('science NCERT CH5.txt','r+',errors = 'ignore')\n",
    "# raw_data=corpus.read()\n",
    "# raw_data = raw_data.lower()\n",
    "# sent_tokens += nltk.tokenize.sent_tokenize(raw_data)\n",
    "# corpus=open('science NCERT CH15.txt','r+',errors = 'ignore')\n",
    "# raw_data=corpus.read()\n",
    "# raw_data = raw_data.lower()\n",
    "# sent_tokens += nltk.tokenize.sent_tokenize(raw_data)\n",
    "# corpus=open('science NCERT CH16.txt','r+',errors = 'ignore')\n",
    "# raw_data=corpus.read()\n",
    "# raw_data = raw_data.lower()\n",
    "# sent_tokens += nltk.tokenize.sent_tokenize(raw_data)\n",
    "\n",
    "# # def remove_non_alpha_numeric_characters(sentence):\n",
    "# #     new_sentence = ''\n",
    "# #     for alphabet in sentence:\n",
    "# #         if alphabet.isalpha() or alphabet == ' ':\n",
    "# #             new_sentence += alphabet\n",
    "# #     return new_sentence\n",
    "# GREETING_INPUTS = [\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\", \"hey there\",\"hey you\",\"yo!\",\"yo yo\"]\n",
    "# GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "# # def greeting(sentence):\n",
    "# #     for word in sentence.split(): # Looks at each word in your sentence\n",
    "# #         if word.lower() in GREETING_INPUTS: # checks if the word matches a GREETING_INPUT\n",
    "# #             return random.choice(GREETING_RESPONSES)\n",
    "# # remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "# # lemmer = nltk.stem.WordNetLemmatizer()\n",
    "# # def LemTokens(tokens):\n",
    "# #     return [lemmer.lemmatize(token) for token in tokens]\n",
    "# # def LemNormalize(text):\n",
    "# #     return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "# # def response(user_response):\n",
    "# #     for words in user_response.split():    \n",
    "# #         robo_response='' # initialize a variable to contain string #add user response to sent_tokens\n",
    "# #         TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english') \n",
    "# #         tfidf = TfidfVec.fit_transform(sent_tokens) #get tfidf value\n",
    "# #         vals = cosine_similarity(tfidf[-1], tfidf) #get cosine similarity value\n",
    "# #         flat = vals.flatten() \n",
    "# #         flat.sort() #sort in ascending order\n",
    "# #         for i in range(10):\n",
    "# #             req_tfidf = flat[-i]\n",
    "# #             idx=vals.argsort()[0][-i]\n",
    "\n",
    "# #             if(req_tfidf!=0) and (req_tfidf!=1):\n",
    "# #                 robo_response = robo_response+sent_tokens[idx]+\"\\n\\n\\n\"\n",
    "# #             else:\n",
    "# #                 print(\"\",end = \"\")\n",
    "# #     robo_response=robo_response.replace(\"~\",\".\")\n",
    "# #     return robo_response\n",
    "# flag=True\n",
    "# while(flag==True):\n",
    "#     user_response = input()\n",
    "#     print(\"_\"*120)\n",
    "#     user_response=user_response.lower()\n",
    "#     #sent_tokens.append(user_response)\n",
    "#     #filepath='Downloads/science NCERT CH3.txt'    \n",
    "#     #corpus=open(filepath,'r+',errors = 'ignore')\n",
    "#     #raw_data=corpus.read()\n",
    "#     #raw_data = raw_data.lower()\n",
    "#     #sent_tokens = nltk.tokenize.sent_tokenize(raw_data)\n",
    "#     #sent_tokens.append(user_response)\n",
    "#     if(user_response!='bye'):\n",
    "#         if(greeting(user_response)!=None):\n",
    "#             print(\"Science BOT: \"+ greeting(user_response))\n",
    "#         else:\n",
    "#             print(response(user_response))\n",
    "#             #sent_tokens.remove(user_response)\n",
    "#     else:\n",
    "#         flag=False\n",
    "#         print(\"Science Bot: Bye! take care and explore more about SCIENCE...\")\n",
    "# sent_tokeens = nltk.tokenize.sent_tokens(raw_tokens)\n",
    "\n",
    "#raw_data = raw_data.lower()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=open('./science NCERT CH2.txt','r+',errors = 'ignore')\n",
    "raw_data=corpus.read()\n",
    "raw_data = raw_data.lower()\n",
    "sent_tokens = nltk.tokenize.sent_tokenize(raw_data)\n",
    "corpus=open('./science NCERT CH3.txt','r+',errors = 'ignore')\n",
    "raw_data=corpus.read()\n",
    "raw_data = raw_data.lower()\n",
    "sent_tokens += nltk.tokenize.sent_tokenize(raw_data)\n",
    "corpus=open('./science NCERT CH4.txt','r+',errors = 'ignore')\n",
    "raw_data=corpus.read()\n",
    "raw_data = raw_data.lower()\n",
    "sent_tokens += nltk.tokenize.sent_tokenize(raw_data)\n",
    "corpus=open('./science NCERT CH6.txt','r+',errors = 'ignore')\n",
    "raw_data=corpus.read()\n",
    "raw_data = raw_data.lower()\n",
    "sent_tokens += nltk.tokenize.sent_tokenize(raw_data)\n",
    "corpus=open('./science NCERT CH7.txt','r+',errors = 'ignore')\n",
    "raw_data=corpus.read()\n",
    "raw_data = raw_data.lower()\n",
    "sent_tokens += nltk.tokenize.sent_tokenize(raw_data)\n",
    "corpus=open('./science NCERT CH1.txt','r+',errors = 'ignore')\n",
    "raw_data=corpus.read()\n",
    "raw_data = raw_data.lower()\n",
    "sent_tokens += nltk.tokenize.sent_tokenize(raw_data)\n",
    "corpus=open('./science NCERT CH8.txt','r+',errors = 'ignore')\n",
    "raw_data=corpus.read()\n",
    "raw_data = raw_data.lower()\n",
    "sent_tokens += nltk.tokenize.sent_tokenize(raw_data)\n",
    "corpus=open('./science NCERT CH9.txt','r+',errors = 'ignore')\n",
    "raw_data=corpus.read()\n",
    "raw_data = raw_data.lower()\n",
    "sent_tokens += nltk.tokenize.sent_tokenize(raw_data)\n",
    "corpus=open('./science NCERT CH5.txt','r+',errors = 'ignore')\n",
    "raw_data=corpus.read()\n",
    "raw_data = raw_data.lower()\n",
    "sent_tokens += nltk.tokenize.sent_tokenize(raw_data)\n",
    "corpus=open('./science NCERT CH15.txt','r+',errors = 'ignore')\n",
    "raw_data=corpus.read()\n",
    "raw_data = raw_data.lower()\n",
    "sent_tokens += nltk.tokenize.sent_tokenize(raw_data)\n",
    "corpus=open('./science NCERT CH16.txt','r+',errors = 'ignore')\n",
    "raw_data=corpus.read()\n",
    "raw_data = raw_data.lower()\n",
    "sent_tokens += nltk.tokenize.sent_tokenize(raw_data)\n",
    "\n",
    "# background: url('https://d2r55xnwy6nx47.cloudfront.net/uploads/2019/03/AI-Science_Lede_Fullwidth.jpg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".app-subtitle {\n",
       "    font-size: 1.5em;\n",
       "}\n",
       "\n",
       ".app-subtitle a {\n",
       "    color: #106ba3;\n",
       "}\n",
       "\n",
       ".app-subtitle a:hover {\n",
       "    text-decoration: underline;\n",
       "}\n",
       "\n",
       ".app-sidebar p {\n",
       "    margin-bottom: 1em;\n",
       "    line-height: 1.7;\n",
       "}\n",
       "\n",
       ".app-sidebar a {\n",
       "    color: #106ba3;\n",
       "}\n",
       "\n",
       ".app-sidebar a:hover {\n",
       "    text-decoration: underline;\n",
       "}\n",
       "\n",
       ".abc {\n",
       "  background: url('back.jpg');\n",
       "  width:100%;\n",
       "  height: 100%;\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\\\n",
    "<style>\n",
    ".app-subtitle {\n",
    "    font-size: 1.5em;\n",
    "}\n",
    "\n",
    ".app-subtitle a {\n",
    "    color: #106ba3;\n",
    "}\n",
    "\n",
    ".app-subtitle a:hover {\n",
    "    text-decoration: underline;\n",
    "}\n",
    "\n",
    ".app-sidebar p {\n",
    "    margin-bottom: 1em;\n",
    "    line-height: 1.7;\n",
    "}\n",
    "\n",
    ".app-sidebar a {\n",
    "    color: #106ba3;\n",
    "}\n",
    "\n",
    ".app-sidebar a:hover {\n",
    "    text-decoration: underline;\n",
    "}\n",
    "\n",
    ".abc {\n",
    "  background: url('back.jpg');\n",
    "  width:100%;\n",
    "  height: 100%;\n",
    "}\n",
    "\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = [\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\", \"hey there\",\"hey you\",\"yo!\",\"yo yo\",'select most asked questions']\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "class App:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "        self.lemmer = nltk.stem.WordNetLemmatizer()\n",
    "        self._input_text = widgets.Text(\n",
    "            value='',\n",
    "            placeholder='Type Your Question',\n",
    "            description='',\n",
    "            disabled=False\n",
    "        )\n",
    "        self._input_text.on_submit(self.on_submit)\n",
    "        self._input_text.layout = widgets.Layout(width='80%')\n",
    "        self.questions = ['Select Most Asked Questions','What is Photosynthesis','Name the plant hormones that promotes growth','What is multiple fission? ','Give Examples of homologous organs','What is biological magnification','What are indicators of water pollution','What is sustainable development','What is metal oxide?','What is the melting point of ethanoic acid?','What is neutralisation reaction?','What is rock salt?','What is homologous series?','Give demerits of Mendeleev periodic law.','What is newland law of octave','What are exothermic reaction','Give different types of chemical reactions.']\n",
    "        self._q_dropdown = self._create_indicator_dropdown(self.questions, 0, '')\n",
    "        self._q_dropdown.layout = widgets.Layout(width='80%')\n",
    "        self._c1 = widgets.HTML(\n",
    "            '<p style=\"color:white;\">Chemical Reactions and Equations | Acids, Bases and Salts | Metals and Non-Metals | Carbon and its Compounds | Periodic Classification of Elements | Life Processes | Control and Coordination | How does organisms reproduce | Heredity and Evolution | Our Environment</p>',\n",
    "        )\n",
    "        self._c1.layout = widgets.Layout(width='80%')\n",
    "        self._chapter = widgets.HTML(value='<p></p>')\n",
    "        self._result_html = widgets.HTML(value='<p></p>')\n",
    "        self._result_html.layout = widgets.Layout(width='80%')\n",
    "        self.container = widgets.VBox([\n",
    "            widgets.HTML(\n",
    "                (\n",
    "                    '<h1 align=\"center\" style=\"color:white;\">Science Bot</h1>'\n",
    "#                     '<p align=\"center\">Put Your Question Here</p>'\n",
    "                ), \n",
    "                layout=widgets.Layout(margin='0 0 1em 0')\n",
    "            ),\n",
    "            self._c1,\n",
    "            self._input_text, \n",
    "            self._q_dropdown,\n",
    "            self._result_html\n",
    "        ], layout=widgets.Layout(flex='1 1 auto', margin='0 auto 0 auto', max_width='100%', align_items='center',height='1000px'))\n",
    "        \n",
    "    @classmethod\n",
    "    def from_url(cls):\n",
    "        return cls()\n",
    "        \n",
    "    def _create_indicator_dropdown(self, indicators, initial_index, desc):\n",
    "        dropdown = widgets.Dropdown(options=indicators, value=indicators[initial_index], description=desc)\n",
    "        dropdown.observe(self._on_change, names=['value'])\n",
    "        return dropdown\n",
    "        \n",
    "    def _on_change(self, _):\n",
    "        self.on_submit(self._q_dropdown)\n",
    "    \n",
    "    def greeting(self, sentence):\n",
    "        for word in sentence.split(): # Looks at each word in your sentence\n",
    "            if word.lower() in GREETING_INPUTS: # checks if the word matches a GREETING_INPUT\n",
    "                return random.choice(GREETING_RESPONSES)\n",
    "    \n",
    "    def LemTokens(self, tokens):\n",
    "        return [self.lemmer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    def LemNormalize(self, text):\n",
    "        return self.LemTokens(nltk.word_tokenize(text.lower().translate(self.remove_punct_dict)))\n",
    "    \n",
    "    def response(self, user_response):\n",
    "        all_answers = 0\n",
    "        response_list = []\n",
    "        for words in user_response.split():    \n",
    "            robo_response=''\n",
    "            results = '<b><p>'\n",
    "            # initialize a variable to contain string #add user response to sent_tokens\n",
    "            TfidfVec = TfidfVectorizer(tokenizer=self.LemNormalize, stop_words='english') \n",
    "            tfidf = TfidfVec.fit_transform(sent_tokens) #get tfidf value\n",
    "            vals = cosine_similarity(tfidf[-1], tfidf) #get cosine similarity value\n",
    "            flat = vals.flatten() \n",
    "            flat.sort() #sort in ascending order\n",
    "\n",
    "        #i = 2\n",
    "            for i in range(10):\n",
    "                req_tfidf = flat[-i]\n",
    "                idx=vals.argsort()[0][-i]\n",
    "\n",
    "                if(req_tfidf!=0) and (req_tfidf!=1):\n",
    "                    robo_response = robo_response + sent_tokens[idx]+\"\\n\\n\\n\"\n",
    "                    response_list.append(sent_tokens[idx].replace(\"~\",\".\"))\n",
    "                    all_answers+= 1\n",
    "                else:\n",
    "                    print(\"\",end = \"\")\n",
    "        dictionary = {}\n",
    "        for i in range(all_answers):\n",
    "            correct_answer = 0\n",
    "            for j in nltk.tokenize.word_tokenize(user_response):\n",
    "                if j in nltk.tokenize.word_tokenize(response_list[i]):\n",
    "                    correct_answer += 1\n",
    "            dictionary[response_list[i]] = correct_answer\n",
    "\n",
    "        sort_dict = sorted(dictionary.items(), key=operator.itemgetter(1))\n",
    "        sort_dict.reverse()\n",
    "        bolda = 0\n",
    "        for i in sort_dict:\n",
    "            if len(i[0]) > 0:\n",
    "                if bolda < 3:\n",
    "                    results = results + '</p></b><p style=\"color: #fff; background: rgb(0, 188, 212); padding:20px;\"><b>' + i[0]\n",
    "                    bolda = bolda + 1\n",
    "                elif bolda == 3:\n",
    "                    results = results + '</p></b><p style=\"color: white; background: black; padding:20px;\">' + i[0]\n",
    "                    bolda = bolda + 1\n",
    "                else:\n",
    "                    results = results + '</p><p style=\"color: white; background: black; padding:20px;\">' + i[0]\n",
    "#             print(i[0])\n",
    "#             print(\"------\")\n",
    "        result = results + '</p>'\n",
    "        #print(result)        \n",
    "        self._result_html.value = result\n",
    "        robo_response=robo_response.replace(\"~\",\".\")\n",
    "        return robo_response\n",
    "    \n",
    "    def on_submit(self, _input):\n",
    "        user_input = str(_input.value)\n",
    "        user_input = user_input.lower()\n",
    "        sent_tokens.append(user_input)\n",
    "        if(user_input != 'bye'):\n",
    "            isGreeting = self.greeting(user_input)\n",
    "            if(self.greeting(user_input) != None):\n",
    "                print(\"Science BOT: \"+ isGreeting)\n",
    "            else:\n",
    "                result = self.response(user_input)\n",
    "                #print(result)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553bfd56f81a47faa505c4328a94e12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1 align=\"center\" style=\"color:white;\">Science Bot</h1>', layout=Layout(margin='0 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishesh\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "app = App.from_url()\n",
    "\n",
    "# fs = Fullscreen(app.container)\n",
    "# display(fs)\n",
    "\n",
    "app.container.add_class('abc')\n",
    "app.container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
